{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8937d254",
   "metadata": {},
   "source": [
    "\n",
    "![Biofilm image](../images/Biofilm_Website_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c1764",
   "metadata": {},
   "source": [
    "# SubModule #6: Running workflows at scale with AWS Batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0048822c-bce5-47b7-90b7-1cb02234fa5b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "One of the greatest benefits that cloud computing affords you is the ability to pay only for the resources that you need and only as long as you need them. In the previous submodules we have relied on an individual VM that we manually turn on at the beginning of our work and off at the end. If we forgot to turn the VM off at the end of our work, we could incur an unecessarily large bill. Similarly, we might have certain stages of our analysis that require huge computational resources and others that require very little. Rather than stopping and resizing the VM at each stage, we can instead rely on the AWS Batch scheduler to reserve the resources required for each analysis. The resources are automatically shut down and deleted at the end, so you don't pay any additional cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b352b-f792-4525-a497-596d0ed29cc1",
   "metadata": {},
   "source": [
    "In the previous submodules, many of the steps had to be run one at a time in the correct order. That is traditionally how bioinformatics pipelines have been execute. The arrivale of workflow managers like [Nextflow](https://www.nextflow.io/) have changed that. Nextflow allows developers to chain many commands together into a mature pipeline. The flow input and output files throughout the pipeline governs the order of execution to ensure that a particular step is not launched if its input has not yet been generated by a previous step. Nextflow has aggregated a growing number of curate workflows into a library of commonly used pipelines called [nf-core](https://nf-co.re/). From nf-core, you can easily launch workflows to run canonical tools for processes like RNA-Seq, methyl-seq, and in our case, 16s metagenomic sequencing. For this submodule, we will run a classic 16s metagenomics analysis suite using nf-core's [ampliseq](https://nf-co.re/ampliseq) pipeline. We have made some adjustments from the original scripts so that it can run on AWS Batch, so we will pull it from our Cloud Storage Bucket rather than launch it from GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad4630",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "- Understand the benefit of using schedulers like AWS Batch\n",
    "- Identify the key components of a AWS Batch profile in a Nextflow config file\n",
    "- Launch an end-to-end nf-core pipeline on AWS Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d536e0c",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Software**\n",
    "\n",
    "* **Nextflow:** For running the `ampliseq` pipeline.\n",
    "\n",
    "**APIs**\n",
    "\n",
    "* **Amazon S3**  The notebook extensively uses `aws s3` commands, indicating it needs access to Amazon S3 for downloading input files (`*.gz`, `ampliseq`) and potentially for storing intermediate and output files from the Nextflow pipeline.\n",
    "\n",
    "* **AWS Batch Compute Environment and Job Queue:** You must have an AWS Batch compute environment and job queue configured. The CloudFormation template automates this. You can set up one manually following the instructions in the link provided in the notebook, but using the template is *recommended* for ease of setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b487e94f",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "### Install Nextflow\n",
    "\n",
    "Run the following cell to install nextflow version `23.10.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaba65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mamba create  -n nextflow -c bioconda nextflow=23.10.0 -y\n",
    "! mamba install -n nextflow ipykernel -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734d66a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "    <b>Alert: </b> Remember to change your kernel to <b>conda_nextflow</b> to run nextflow.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a939aae",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666d0b7",
   "metadata": {},
   "source": [
    "We need to download some data and our updated version of the ampliseq pipeline. For this submodule, we are using a human fecal microbiome sample from an ulcerative colitis experiment. Information about the sample can be found on its [SRA listing](https://www.ncbi.nlm.nih.gov/sra/?term=SRR24091844)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab091c-4cfd-40c6-8718-3cfb20df6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp s3://nigms-sandbox/nosi-usd-biofilms/  Core_Dataset_Prep/  --exclude \"*\" --include \"*.fastq.gz\"  --recursive\n",
    "! aws s3 cp s3://nigms-sandbox/nosi-usd-biofilms/ampliseq  ./ampliseq/    --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e6e184",
   "metadata": {},
   "source": [
    "### AWS Batch Setup\n",
    "\n",
    "AWS Batch will create the needed permissions, roles and resources to run Nextflow in a serverless manner. You can set up AWS Batch manually or deploy it **automatically** with a stack template. The Launch Stack button below will take you to the cloud formation create stack webpage with the template with required resources already linked. \n",
    "\n",
    "If you prefer to skip manual deployment and deploy automatically in the cloud, click the Launch Stack button below. For a walkthrough of the screens during automatic deployment please click [here](https://github.com/NIGMS/NIGMS-Sandbox/blob/main/docs/HowToLaunchAWSBatch.md). The deployment should take ~5 min and then the resources will be ready for use. \n",
    "\n",
    "[![Launch Stack](../images/LaunchStack.jpg)](https://console.aws.amazon.com/cloudformation/home?#/stacks/new?stackName=aws-batch-nigms&templateURL=https://nigms-sandbox.s3.us-east-1.amazonaws.com/cf-templates/AWSBatch_template.yaml)\n",
    "\n",
    "\n",
    "Before beginning this tutorial, if you do not have required roles, policies, permissions or compute environment and would like to **manually** set those up please click [here](https://github.com/NIGMS/NIGMS-Sandbox/blob/main/docs/AWS-Batch-Setup.md) to set that up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7373dadf-86d6-4755-8cf9-33d7200e0b6e",
   "metadata": {},
   "source": [
    "Below is the `aws` profile that we have added to the nextflow.config file. Config files specify an enormous range of parameters that Nextflow uses to run your workflow. You can see where we have specified AWS Batch as the executor for the pipeline as well as lines that specify `AWS Batch queue`, `AWS region`, `working directory`, and `output directory`. It is important that you update these prior to launching your run in the cell below.\n",
    "\n",
    "```\n",
    "aws {\n",
    "process {\n",
    "        executor = 'awsbatch'\n",
    "        queue = 'nextflow-batch-job-queue'  // Name of your Job queue\n",
    "        container = 'quay.io/nextflow/rnaseq-nf:v1.1'\n",
    "        \n",
    "}\n",
    "workDir = 's3://your_bucket_name/rna-tmp/'    // path of your working directory\n",
    "params.outdir = 's3://your_bucket_name/rna-outputs/'   // path of your output directory\n",
    "\n",
    "fusion.enabled = true\n",
    "wave.enabled = true\n",
    "aws.region = 'us-east-1' // YOUR AWS REGION\n",
    "\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dacfb28",
   "metadata": {},
   "source": [
    "### Run `ampliseq` pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8864a7-c05c-4c8f-af2b-cd5b98a56118",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nextflow run nf-core/ampliseq -r 2.6.0 \\\n",
    "    -profile aws \\\n",
    "    -c ampliseq/nextflow.config \\\n",
    "    --input \"Core_Dataset_Prep/\" \\\n",
    "    --FW_primer GTGYCAGCMGCCGCGGTAA \\\n",
    "    --RV_primer GGACTACNVGGGTWTCTAAT \\\n",
    "    --skip_taxonomy \"true\" \\\n",
    "    --skip_cutadapt \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd89268-1ddc-4a5e-9e3e-c5102f044c25",
   "metadata": {},
   "source": [
    "After the pipeline completes, you can see the output of each stage in the directory you specify as `params.outdir` in the config file. You can easily see the output files generated by fastqc, dada2, and others neatly categorized into its own directory. As you scale the size and number of metagenomic runs that you execute, using AWS Batch with Nextflow will vastly improve the efficiency and reproducibility of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae487f9d-cff3-4a69-8feb-14317d307a90",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6c15c-293a-4d2d-a477-1b47d0bdf0ab",
   "metadata": {},
   "source": [
    "You can see from the progress updates above that each step of the pipeline was given a process name and allocated to appropriate resources. Once that process was done, those resources were shut down and deleted so you don't have to continue paying for them. Output files and logging information was stored safely in the storage buckets we specified in the congif file. Use resource managers like AWS Batch is a great way to scale your workflows as your datasets get bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d6f90a",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Remember to stop your notebook instance when you are done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
