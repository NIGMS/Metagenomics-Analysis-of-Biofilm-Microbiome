{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Biofilm image](../images/Biofilm_Website_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submodule #4: Microbiome Community Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "### Step 5 - Microbial Community and Biofilm Analysis (Amazon Athena): \n",
    "\n",
    "The microbial community analysis relies on comparisons of predicted genes, proteins, and functions with existing previously annotated sequences. Functional profiling provides insights into what functions are carried out by a given microbial community and biofilm. Quorum sensing (QS) is one of the key indicators of a bacterial community's behavior. QS is the regulation of gene expression in response to fluctuations in cell-population density. QS bacteria produce and release chemical signaling molecules called autoinducers that increase in concentration as a function of cell density. The presence of QS signaling does not always guarantee biofilm formation, but this phenomenon has proven to be a reliable marker in several phenotype analyses of biofilms, such as those involved in cancer, dental health, medical devices, corrosion, and environmental biofilms.  Here we use the STRING Database and BLAST+ to search for biofilm signatures in our metagenomic samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "*   Understanding biofilms and quorum sensing\n",
    "*   Extracting and manipulating data using Python and pandas\n",
    "*   Querying biological databases (UniProt, STRING) via APIs and Amazon Athena\n",
    "*   Creating and managing FASTA files\n",
    "*   Utilizing Amazon S3 and Batch for data storage and parallel processing\n",
    "*   Running BLAST+ analysis using Nextflow\n",
    "*   Interpreting BLAST results to identify significant hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "*   **Software:** Python 3 (with pandas), Nextflow (v23.10.0+), `pyathena`.\n",
    "*   **Data:** `pred_metagenome_unstrat.tsv` (PICRUSt2 output).\n",
    "*   **AWS:** Account, IAM roles (S3, Athena, Batch access), configured Athena database (`usd_metagenomics_proteins` with tables `proteins_names`, `proteins_sequences`, `pr_names`), S3 bucket, and AWS Batch job queue (defined in `Blast/nextflow.config`).\n",
    "*   **APIs:** UniProt REST API (publicly accessible).\n",
    "*   **Configuration:**  Correctly configured `Blast/nextflow.config` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "### Query on the STRING Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to carry out our community analysis we will need to gather the protein names of each EC number from our pred_metagenome_unstrat.tsv output that was generated by our PICRUSt2 analysis. To do this we will be using a `for loop` to loop through our file, extracting each EC number and searching it against the Uniprot protein database. All outputs are then added to the **pr_names** list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this can take ~15 min to run, depending on your VM\n",
    "import pandas as pd\n",
    "enzym_list = pd.read_csv('BioMarker_Discovery/picrust2_output/EC_metagenome_out/pred_metagenome_unstrat.tsv',sep='\\t')['function'].values\n",
    "pr_names = []\n",
    "for enzy in enzym_list:\n",
    "    try:\n",
    "        pr_names += list(pd.read_csv('https://rest.uniprot.org/uniprotkb/search?query=(' + enzy +')&format=tsv',sep='\\t')['Entry Name'].values)\n",
    "    except:\n",
    "        print(pd.read_csv('https://rest.uniprot.org/uniprotkb/search?query=(' + enzy +')&format=tsv',sep='\\t')['Entry Name'].values)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pr_names = pd.DataFrame(list(set(pr_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pr_names.to_csv(\"pr_names.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload your data into a s3 bucket in your account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make a bucket if you need one\n",
    "# ! aws s3 mb s3://bucket-for-athena-query-test\n",
    "! aws s3 cp pr_names.csv s3://<bucket-for-athena-query>/pr_names/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up your Athena Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to set up your Athena database in the Athena console before you start this notebook. Follow our [guide](https://github.com/STRIDES/NIHCloudLabAWS/blob/main/docs/create_athena_database.md) to walk you through it:\n",
    "- In step 7 add two tables stored in a public S3 bucket:\n",
    "  - s3://nigms-sandbox/usd_metagenomics_proteins/proteins_sequences/\n",
    "  - s3://nigms-sandbox/usd_metagenomics_proteins/proteins_sequences/\n",
    "- In step 7 add the table stored in your S3 bucket:\n",
    "  - s3://bucket-for-athena-query/pr_names/pr_names.csv\n",
    "- In step 8, give an `AWSGlueServiceRole` and then `Update Choosen IAM Role`, so Glue would have the \"read access\" to above data.\n",
    "- In step 9, Click `Add database`, create a database with the name `usd_metagenomics_proteins` and then choose it as your target database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query this list of proteins against the STRING Database that NIH has hosted as a public database. An alternative database that can be used is the Biofilms Structural Database (BSD), a collection of structural, mutagenesis, kinetics, and inhibition data to understand the processes involved in biofilm formation.\n",
    "\n",
    "You will notice that we run two queries. The first is to collect the proteins IDs associated with the names we recieved from the Uniprot database. The second query is used to extract the sequences that are associated with the protein IDs. We will need both in order to run these sequences through BLAST+. You can see in the code below that the protein IDs and sequences are stored in a dataframe named **df**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query in Amazon Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pyathena\n",
    "! pip install pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from pyathena import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(s3_staging_dir='s3://bucket-for-athena-query-test/',\n",
    "               region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "query = \"\"\"\n",
    "    SELECT * FROM AwsDataCatalog.usd_metagenomics_proteins.proteins_names WHERE protein_name IN (SELECT col1 FROM AwsDataCatalog.usd_metagenomics_proteins.pr_names) \"\"\"\n",
    "df_proteins_names = pd.read_sql(\n",
    "    query, conn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "query = f\"\"\" SELECT protein_id, sequence FROM AwsDataCatalog.usd_metagenomics_proteins.proteins_sequences where protein_id IN \n",
    "(SELECT protein_id FROM AwsDataCatalog.usd_metagenomics_proteins.proteins_names WHERE protein_name IN \n",
    "(SELECT col1 FROM AwsDataCatalog.usd_metagenomics_proteins.pr_names) )\n",
    "    \"\"\"\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Fasta file (.fa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at our dataframe! We can see that we have a column of numbers (our indexes), protein IDs, and sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view our dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before, we will be submitting these sequences to BLAST+. The fastest way to parse multiple seuqences is via a fasta file. Fasta files contain two pieces of information: 1) protein IDs and 2) protein sequences. We need to reformat our dataframe so that it has this information in fasta format. You can read more about fasta files [here](https://www.ncbi.nlm.nih.gov/genbank/fastaformat/).\n",
    "\n",
    "In fasta format, each sequence header that contains the protein ID should start with **'>'** symbol. This is the first change we will make.\n",
    "\n",
    "The second change will be to replace any spaces with underscores within the protein_id column using the `replace` command, `lambda` helps to implement this rule to the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['protein_id'] ='>' + df['protein_id'].astype(str)\n",
    "df['protein_id'] = df['protein_id'].apply(lambda x:x.replace(' ','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view our dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final steps can be done in one command! Notice that our protein IDs and sequences are in one row instead of in separate rows we can us the `.to_csv` command to not only convert our file into a fasta file but to also make any spaces into new lines. The next thing we do is set our indexes and headers to false, this will get rid of the column of numbers and our headers (protein_id and sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a fasta file\n",
    "! mkdir Microbiome_Community_Analysis\n",
    "df.to_csv('Microbiome_Community_Analysis/subset_proteins_seqs.fa',sep=\"\\n\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the Fasta File to Your Own Bucket \n",
    "Finally we can add our file to a Amazon S3 Bucket!\n",
    "\n",
    "To create a bucket, use the following command, **be sure to replace `<REPLACE_W_BUCKET_NAME_>` with your own unique bucket name**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env BUCKET=<REPLACE_W_BUCKET_NAME_>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use following command to make a bucket. The mb command stands for 'make bucket'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 mb s3://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the `cp` command to copy our subset_proteins_seqs.fa file into our bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp Microbiome_Community_Analysis/subset_proteins_seqs.fa s3://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running BLAST+ with Nextflow through AWS Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLAST+ (BLASTp) allows us to query multiple sequences through the BLAST database and will then retrieve a collection of hits in our case protein IDs with sequences similar to our collection. The Nextflow pipeline runs the following steps:\n",
    "\n",
    "1. Make a custom BLAST database based on our premade fasta file from UniprotKB, this our quorum sensing protein collection (uniprot-download_true_format_fasta_includeIsoform_true-2022.09.23-02.40.31.85.fasta)\n",
    "2. Query our own fasta file against the custom database to see if we have any plausible hits\n",
    "3. Add headers to our output file to better understand the results\n",
    "\n",
    "We will be submitting these commands via Nextflow to AWS Batch. This feature allows us to run our BLAST search in parallel by starting up multiple VMs each one running a different job, allowing us to receive our output faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Batch Setup\n",
    "\n",
    "AWS Batch will create the needed permissions, roles and resources to run Nextflow in a serverless manner. You can set up AWS Batch manually or deploy it **automatically** with a stack template. The Launch Stack button below will take you to the cloud formation create stack webpage with the template with required resources already linked. \n",
    "\n",
    "If you prefer to skip manual deployment and deploy automatically in the cloud, click the Launch Stack button below. For a walkthrough of the screens during automatic deployment please click [here](https://github.com/NIGMS/NIGMS-Sandbox/blob/main/docs/HowToLaunchAWSBatch.md). The deployment should take ~5 min and then the resources will be ready for use. \n",
    "\n",
    "[![Launch Stack](../images/LaunchStack.jpg)](https://console.aws.amazon.com/cloudformation/home?#/stacks/new?stackName=aws-batch-nigms&templateURL=https://nigms-sandbox.s3.us-east-1.amazonaws.com/cf-templates/AWSBatch_template.yaml)\n",
    "\n",
    "\n",
    "Before beginning this tutorial, if you do not have required roles, policies, permissions or compute environment and would like to **manually** set those up please click [here](https://github.com/NIGMS/NIGMS-Sandbox/blob/main/docs/AWS-Batch-Setup.md) to set that up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the parameters as desired in `Blast/nextflow.config` file\n",
    " - Name of your **AWS Batch Job Queue** \n",
    " - **Inputs** : \n",
    "     - **INPUT**: Path of the uniprot-download_true_format_fasta_includeIsoform_true-2022.09.23-02.40.31.85.fasta file to create a new BLAST database (this will be automatically pulled from the public bucket) \n",
    "     - **QUERY**: Path to where subset_proteins_seqs.fa was saved in your bucket\n",
    " - **OUTPUT_FILE** : Path to the output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Nextflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! mamba create  -n nextflow -c bioconda nextflow=23.10.0 -y\n",
    "! mamba install -n nextflow ipykernel -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "    <b>Alert: </b> Remember to change your kernel to <b>conda_nextflow</b> to run nextflow.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Nextflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nextflow run Blast/main.nf -profile aws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing BLAST Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align: center;\">BLAST Results Header Names and Descriptions</h4>\n",
    "\n",
    "| Name      | Description |\n",
    "| ----------- | ----------- |\n",
    "| qseqid     |  query or source (e.g., gene) sequence id |\n",
    "| sseqid   | subject  or target (e.g., reference genome) sequence id |\n",
    "| pident     |  percentage of identical matches |\n",
    "| length   | alignment length (sequence overlap) |\n",
    "| mismatch     |  number of mismatches |\n",
    "| gapopen   | number of gap openings |\n",
    "| qstart     |  start of alignment in query |\n",
    "| qend   | end of alignment in query |\n",
    "| sstart     |  start of alignment in subject |\n",
    "| send  | end of alignment in subject |\n",
    "| evalue     |  expect value |\n",
    "| bitscore  | bit score|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to look at our results as is we would see many more than 30 hits but many can have high e-values and/or have a low percent similarity (pident). To help us evaluate which hits are useful in our analysis we will filter our results based on pident and evalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_unfil = pd.read_csv(\"Microbiome_Community_Analysis/proteins_blastp_results.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_unfil.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's filter by percentage of identical matches that equal 100% and sort by the lowest e-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting rows based on condition\n",
    "results_100= results_unfil[results_unfil['pident'] == 100]\n",
    "results_100.sort_values(by=['evalue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have several protein IDs that have a 100% match to some of our sequences. Now let's look at their e-values. We want e-values that are very close to 0 becuase it tells us our hit is very significant. That is exactly what we see from our table above. With these results we can conclude that there are biofilm markers within our dataset. To identify these sequences you can look up the seqid in the Uniprot Database: https://www.uniprot.org/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "Run the cell below to check your knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the command below to view the quiz\n",
    "from IPython.display import IFrame\n",
    "IFrame(\"../Quiz/QS15.html\", width=800, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module you learned how to identify community markers relevant to biofilm formation. From our microbiome analysis module (submodule03), you extracted the most relevant markers by cross matching the predictive markers from PICRUSt2 with existing community knowledge bases (e.g. Uniprot, STRING Database). In this submodule, we focused on quorum sensing proteins as the core marker to identify the biofilm signature. Itâ€™s important to note that, in some cases quorum sensing is not enough to characterize biofilm formation. The BLAST+ output file allows us to formulate testable hypothesis. For example, in our use case dataset, the highest percentage of identical matches was 100% and our top 5 protein IDs were Q9I4X3, P08987, P13470, O31775, P33883. These show that there are biofilm markers within our dataset, however expert curation and experimental validation is needed to confirm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Remember to stop your notebook instance when you are done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_nextflow",
   "language": "python",
   "name": "conda_nextflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
