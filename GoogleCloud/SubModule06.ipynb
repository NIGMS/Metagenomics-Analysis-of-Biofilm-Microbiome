{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087c13b2",
   "metadata": {},
   "source": [
    "\n",
    "![Biofilm image](../images/Biofilm_Website_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6de2a",
   "metadata": {},
   "source": [
    "# SubModule #5: Running workflows at scale with Google Batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95027682",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "One of the greatest benefits that cloud computing affords you is the ability to pay only for the resources that you need and only as long as you need them. In the previous submodules we have relied on an individual VM that we manually turn on at the beginning of our work and off at the end. If we forgot to turn the VM off at the end of our work, we could incur an unecessarily large bill. Similarly, we might have certain stages of our analysis that require huge computational resources and others that require very little. Rather than stopping and resizing the VM at each stage, we can instead rely on the AWS Batch scheduler to reserve the resources required for each analysis. The resources are automatically shut down and deleted at the end, so you don't pay any additional cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc99fb",
   "metadata": {},
   "source": [
    "In the previous submodules, many of the steps had to be run one at a time in the correct order. That is traditionally how bioinformatics pipelines have been execute. The arrivale of workflow managers like [Nextflow](https://www.nextflow.io/) have changed that. Nextflow allows developers to chain many commands together into a mature pipeline. The flow input and output files throughout the pipeline governs the order of execution to ensure that a particular step is not launched if its input has not yet been generated by a previous step. Nextflow has aggregated a growing number of curate workflows into a library of commonly used pipelines called [nf-core](https://nf-co.re/). From nf-core, you can easily launch workflows to run canonical tools for processes like RNA-Seq, methyl-seq, and in our case, 16s metagenomic sequencing. For this submodule, we will run a classic 16s metagenomics analysis suite using nf-core's [ampliseq](https://nf-co.re/ampliseq) pipeline. We have made some adjustments from the original scripts so that it can run on AWS Batch, so we will pull it from our Cloud Storage Bucket rather than launch it from GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316e60f",
   "metadata": {},
   "source": [
    "## Learning Objectives:\n",
    "- Understand the benefit of using schedulers like AWS Batch\n",
    "- Identify the key components of a AWS Batch profile in a Nextflow config file\n",
    "- Launch an end-to-end nf-core pipeline on AWS Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e9c9bc",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Software**\n",
    "\n",
    "* **Nextflow:** For running the `ampliseq` pipeline.\n",
    "\n",
    "\n",
    "**APIs:**\n",
    "\n",
    "* **Google Cloud Storage (GCS) API:**  The notebook extensively uses `gsutil` commands, indicating it needs access to Google Cloud Storage for downloading input files (`*.gz`, `ampliseq`) and potentially for storing intermediate and output files from the Nextflow pipeline. \n",
    "\n",
    "* **Google Compute Engine API (if using Nextflow outside NIH Cloud Lab):** If the user runs the Nextflow pipeline outside the NIH Cloud Lab environment, the Google Compute Engine API is required for Google Batch, which is used to run the pipeline in a serverless fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f51096a",
   "metadata": {},
   "source": [
    "## Get Started\n",
    "\n",
    "### Install Nextflow\n",
    "\n",
    "Run the following cell to install nextflow version `23.10.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c705df",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mamba create  -n nextflow -c bioconda nextflow=23.10.0 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4f349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nextflow to the PATH\n",
    "import os\n",
    "os.environ['PATH'] = '/opt/conda/envs/nextflow/bin:' + os.environ['PATH'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ac1582",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d6464-e2f9-40c0-a71e-4908660cfecc",
   "metadata": {},
   "source": [
    "First, we need to download some data and our updated version of the ampliseq pipeline. For this submodule, we are using a human fecal microbiome sample from an ulcerative colitis experiment. Information about the sample can be found on its [SRA listing](https://www.ncbi.nlm.nih.gov/sra/?term=SRR24091844)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab091c-4cfd-40c6-8718-3cfb20df6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p Core_Dataset_Prep\n",
    "! gsutil -m cp gs://nigms-sandbox/nosi-usd-biofilms/*.fastq.gz Core_Dataset_Prep/\n",
    "! gsutil -m cp -r gs://nigms-sandbox/nosi-usd-biofilms/ampliseq ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6293fbc4",
   "metadata": {},
   "source": [
    "### Google Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7373dadf-86d6-4755-8cf9-33d7200e0b6e",
   "metadata": {},
   "source": [
    "Below is the gcb profile that we have added to the nextflow.config file. Config files specify an enormous range of parameters that Nextflow uses to run your workflow. You can see where we have specified Google Batch as the executor for the pipeline as well as lines that specify a working directory, an output directory, and a project ID. It is important that you update these with paths to your bucket and project ID prior to launching your run in the cell below.\n",
    "\n",
    "```\n",
    "gcb {\n",
    "        process.executor = 'google-batch'\n",
    "        workDir = 'gs://<your_bucket>/gcb'\n",
    "        google.location = 'us-central1'\n",
    "        google.region  = 'us-central1'\n",
    "        google.project = '<your_project_id>'\n",
    "        params.outdir = 'gs://<your_bucket>/gcb/outdir'\n",
    "        process.machineType = 'c2d-highmem-16'\n",
    "     }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3198353",
   "metadata": {},
   "source": [
    "### Run `ampliseq` pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15f9bc-2491-47ed-9b3f-5cfaf6d6d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nextflow run nf-core/ampliseq -r 2.6.0 \\\n",
    "    -profile gcb \\\n",
    "    -c ampliseq/nextflow.config \\\n",
    "    --input \"Core_Dataset_Prep/\" \\\n",
    "    --FW_primer GTGYCAGCMGCCGCGGTAA \\\n",
    "    --RV_primer GGACTACNVGGGTWTCTAAT \\\n",
    "    --skip_taxonomy \"true\" \\\n",
    "    --skip_cutadapt \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd89268-1ddc-4a5e-9e3e-c5102f044c25",
   "metadata": {},
   "source": [
    "After the pipeline completes, you can see the output of each stage in the directory you specify as `params.outdir` in the config file. You can easily see the output files generated by fastqc, dada2, and others neatly categorized into its own directory. As you scale the size and number of metagenomic runs that you execute, using GLS with Nextflow will vastly improve the efficiency and reproducibility of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae487f9d-cff3-4a69-8feb-14317d307a90",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6c15c-293a-4d2d-a477-1b47d0bdf0ab",
   "metadata": {},
   "source": [
    "You can see from the progress updates above that each step of the pipeline was given a process name and allocated to appropriate resources. Once that process was done, those resources were shut down and deleted so you don't have to continue paying for them. Output files and logging information was stored safely in the storage buckets we specified in the congif file. Use resource managers like Google Batch is a great way to scale your workflows as your datasets get bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc65b83",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Remember to stop your notebook instance when you are done!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
