{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5126e999-6f0f-4b64-a309-45d79d262e3e",
   "metadata": {},
   "source": [
    "# SubModule #5: Running workflows at scale with Google Batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddf8473-1fe5-47e6-b7e2-a926f4d947dd",
   "metadata": {},
   "source": [
    "# Learning Objectives:\n",
    "- Understand the benefit of using schedulers like Google Batch\n",
    "- Identify the key components of a Google Batch profile in a Nextflow config file\n",
    "- Launch an end-to-end nf-core pipeline on Google Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0048822c-bce5-47b7-90b7-1cb02234fa5b",
   "metadata": {},
   "source": [
    "One of the greatest benefits that cloud computing affords you is the ability to pay only for the resources that you need and only as long as you need them. In the previous submodules we have relied on an individual VM that we manually turn on at the beginning of our work and off at the end. If we forgot to turn the VM off at the end of our work, we could incur an unecessarily large bill. Similarly, we might have certain stages of our analysis that require huge computational resources and others that require very little. Rather than stopping and resizing the VM at each stage, we can instead rely on the Google Batch scheduler to reserve the resources required for each analysis. The resources are automatically shut down and deleted at the end, so you don't pay any additional cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b352b-f792-4525-a497-596d0ed29cc1",
   "metadata": {},
   "source": [
    "In the previous submodules, many of the steps had to be run one at a time in the correct order. That is traditionally how bioinformatics pipelines have been execute. The arrivale of workflow managers like [Nextflow](https://www.nextflow.io/) have changed that. Nextflow allows developers to chain many commands together into a mature pipeline. The flow input and output files throughout the pipeline governs the order of execution to ensure that a particular step is not launched if its input has not yet been generated by a previous step. Nextflow has aggregated a growing number of curate workflows into a library of commonly used pipelines called [nf-core](https://nf-co.re/). From nf-core, you can easily launch workflows to run canonical tools for processes like RNA-Seq, methyl-seq, and in our case, 16s metagenomic sequencing. For this submodule, we will run a classic 16s metagenomics analysis suite using nf-core's [ampliseq](https://nf-co.re/ampliseq) pipeline. We have made some adjustments from the original scripts so that it can run on Google Batch, so we will pull it from our Cloud Storage Bucket rather than launch it from GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d6464-e2f9-40c0-a71e-4908660cfecc",
   "metadata": {},
   "source": [
    "First, we need to download some data and our updated version of the ampliseq pipeline. For this submodule, we are using a human fecal microbiome sample from an ulcerative colitis experiment. Information about the sample can be found on its [SRA listing](https://www.ncbi.nlm.nih.gov/sra/?term=SRR24091844)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab091c-4cfd-40c6-8718-3cfb20df6ac8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil -m cp gs://nigms-sandbox/nosi-usd-biofilms/*.fastq.gz Core_Dataset_Prep/\n",
    "!gsutil -m cp -r gs://nigms-sandbox/nosi-usd-biofilms/ampliseq ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a312f-faaa-4b56-954d-f287e6234082",
   "metadata": {},
   "source": [
    "Nextflow requires Java, so we install it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4dd24-ebe9-465d-b8bd-7d062d2c7c87",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#First install java\n",
    "!sudo apt update\n",
    "!sudo apt-get install default-jdk -y\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379f42a-a88f-4360-9bfe-0f95804eae59",
   "metadata": {},
   "source": [
    "Next, we specify versions and platforms for Nextflow, then install it. The nextflow executable sits in our home directory and can be easily run by typing `./nextflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4065dc-380b-4901-b837-ac7c067f0630",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Specify nexflow version and platfrom\n",
    "! export NXF_VER=21.10.0\n",
    "! export NXF_MODE=google\n",
    "#Install nexflow, make it exceutable, and update it\n",
    "! curl https://get.nextflow.io | bash\n",
    "! chmod +x nextflow\n",
    "! ./nextflow self-update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7373dadf-86d6-4755-8cf9-33d7200e0b6e",
   "metadata": {},
   "source": [
    "Below is the gcb profile that we have added to the nextflow.config file. Config files specify an enormous range of parameters that Nextflow uses to run your workflow. You can see where we have specified Google Batch as the executor for the pipeline as well as lines that specify a working directory, an output directory, and a project ID. It is important that you update these with paths to your bucket and project ID prior to launching your run in the cell below.\n",
    "\n",
    "```\n",
    "gcb{\n",
    "        process.executor = 'google-batch'\n",
    "        workDir = 'gs://<your_bucket>/gcb'\n",
    "        google.location = 'us-central1'\n",
    "        google.region  = 'us-central1'\n",
    "        google.project = '<your_project_id>'\n",
    "        params.outdir = 'gs://<your_bucket>/gcb/outdir'\n",
    "        process.machineType = 'c2d-highmem-16'\n",
    "     }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15f9bc-2491-47ed-9b3f-5cfaf6d6d195",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ./nextflow run nf-core/ampliseq \\\n",
    "    -profile gcb \\\n",
    "    -c ampliseq/nextflow.config \\\n",
    "    --input_folder \"Core_Dataset_Prep/\" \\\n",
    "    --FW_primer GTGYCAGCMGCCGCGGTAA \\\n",
    "    --RV_primer GGACTACNVGGGTWTCTAAT \\\n",
    "    --skip_taxonomy \"true\" \\\n",
    "    --skip_cutadapt \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd89268-1ddc-4a5e-9e3e-c5102f044c25",
   "metadata": {},
   "source": [
    "After the pipeline completes, you can see the output of each stage in the directory you specify as `params.outdir` in the config file. You can easily see the output files generated by fastqc, dada2, and others neatly categorized into its own directory. As you scale the size and number of metagenomic runs that you execute, using GLS with Nextflow will vastly improve the efficiency and reproducibility of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae487f9d-cff3-4a69-8feb-14317d307a90",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6c15c-293a-4d2d-a477-1b47d0bdf0ab",
   "metadata": {},
   "source": [
    "You can see from the progress updates above that each step of the pipeline was given a process name and allocated to appropriate resources. Once that process was done, those resources were shut down and deleted so you don't have to continue paying for them. Output files and logging information was stored safely in the storage buckets we specified in the congif file. Use resource managers like Google Batch is a great way to scale your workflows as your datasets get bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b19ea-7a80-4f20-826f-f06698016ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
